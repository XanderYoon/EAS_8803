{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "In general, transfer learning is a machine learning technique that applies a model trained on a certain task to a new, related task.\n",
        "\n",
        "If the task is sufficiently similar, the model will optimize the new task more efficiently using transfer learning than training from scratch (random weights). Thus, less data is required.\n",
        "\n",
        "[[1]](#1)"
      ],
      "metadata": {
        "id": "7JQG1iCM0cJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transferability in Neural Networks\n",
        "The layers of a neural network transition from being generic to task-specific. Understanding this transition is quintessential to apply a pre-trained model to a different task.\n",
        "\n",
        "Yosinski et al. empirically studied this transition. They found that, for a neural network trained on natural images, the first layer learned *general* features that are applicable to many datasets or tasks.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/081651b38ff7533550a3adfc1c00da333a8fe86c/4-Figure1-1.png\" width=\"700\"/>\n",
        "</div>\n",
        "\n",
        "First, they trained two networks of the same architecture, each on different halves of ImageNet.\n",
        "\n",
        "Then, they initialized the first three $n$ of a new network with the weights of B and the rest were initialized randomly. They trained this network on task B by either freezing (BnB) or fine-tuning the first three layers (BnB+). They did the same in another network, but with initializing the first $n$ layers with the weights of model A, rather than that of model B.\n",
        "\n",
        "In the diagram above, $n=3$.\n",
        "\n",
        "The results are shown below.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/081651b38ff7533550a3adfc1c00da333a8fe86c/5-Figure2-1.png\" width=\"700\"/>\n",
        "</div>\n",
        "In the top plot, each marker represents the average accuracy over the validation set. Each BnB and AnB have four points because trials were ran on four different A/B dataset splits. In the bottom plot, the lines connect the means of each treatment. Dark and light blue/red refer to B/AnB and B/AnB+, respectively. The text above each line summarizes the interpretation delineated in section 4.1 of the paper.\n",
        "\n",
        "As we can see, transferring features and then fine-tuning them surprisingly results in networks that generalize better than those trained directly on the target dataset.\n",
        "\n",
        "[[2]](#2), [[3]](#3)"
      ],
      "metadata": {
        "id": "aj_U-iUa2p9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Application of Transfer Learning in CNNs\n",
        "When using a pre-trained model for a new task, the last fully-connected layer must be replaced with one that is appropriate for the new task. In the above experiment, the old and new tasks were the same, thus no replacement was required.\n",
        "\n",
        "For example, ResNet, being trained on ImageNet, takes an image and outputs 1 out of 1000 possible classifications. However, if we wished to use ResNet to output whether or not an image is that of a dog, we would have to replace the last layer with 1000 neurons with one with 1 neuron.\n",
        "\n",
        "[[4]](4)"
      ],
      "metadata": {
        "id": "4Ir6KloXHvak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Transfer Learning Guidelines\n",
        "Generally, transfer learning follows the workflow below:\n",
        "1. Take layers from a previously trained model.\n",
        "2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n",
        "3. Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n",
        "4. Train the new layers on your dataset.\n",
        "5. (optionally) Unfreeze the part/all of the model and continue training at a low learning rate.\n",
        "\n",
        "For more specific guidelines, one must consider two important factors: the size of the new dataset and the similarity of the new dataset to the original dataset.\n",
        "$$\n",
        "\\begin{array}{|c|c|c|}\n",
        "    \\hline\n",
        "    &\\text{Similar to ImageNet}&\\text{Different from ImageNet}\\\\\n",
        "    \\hline\n",
        "    \\text{Small Dataset}&\\text{Freeze all layers & retrain only classifier}&\\text{Tough situation: Freeze lower layers and train classifier on top of those lower layers (discard deeper layers)}\\\\\n",
        "    \\hline\n",
        "    \\text{Large Dataset}&\\text{Freeze lower layers & retrain FC and few conv layers}&\\text{Retrain a larger number of layers}\\\\\n",
        "    \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "[[4]](#4)"
      ],
      "metadata": {
        "id": "GOzePV1W-xlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. H. Bharadwaj, T. LaGrow. Transfer Learning for Boosting Neural Network Performance. 2024. Available: https://sites.gatech.edu/omscs7641/2024/02/07/transfer-learning-for-boosting-neural-network-performance/\n",
        "2. H. Venkateswara. Introduction to Deep Learning. 2024.\n",
        "3. J. Yosinski et al. How transferable are features in deep neural networks?. 2014. Available; https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf\n",
        "4. A. Karpathy et al. CS231n: Transfer Learning. 2016. Available: https://cs231n.github.io/transfer-learning/\n",
        "5. F. Chollet. 2023. Transfer learning & fine-tuning. Available: https://keras.io/guides/transfer_learning/#the-typical-transferlearning-workflow"
      ],
      "metadata": {
        "id": "YC2mU3oD0wSW"
      }
    }
  ]
}