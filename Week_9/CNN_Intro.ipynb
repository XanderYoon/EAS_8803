{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Convolutional neural networks (CNNs) are a variant of traditional neural networks specially designed for spatial data with a vastly reduced number of parameters."
      ],
      "metadata": {
        "id": "oCrYgtpERTyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intuition\n",
        "In 1959, Hubel & Wiesel found that neurons in the cerebral cortex of a cat responded to \"edges\", lines presented in certain locations or orientations. This was discovered after they had difficulty in detecting responses from neurons to traditional visual stimuli, such as dots.\n",
        "\n",
        "Similarly, we can design neural networks which detect and process edges.\n"
      ],
      "metadata": {
        "id": "-Omy9cybjOgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[4]](#4)"
      ],
      "metadata": {
        "id": "2cOSX_vRXwLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's An Edge?\n",
        "Edges can be defined as a discontinuity in the gradient of image intensity.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MARSHALL/edge_types.gif\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Such discontinuities can be detected by observing the gradient. For example, the gradient of (a) would be some $x>0$ before instantly changing to $-x$. At this jump discontinuity, we observe an edge.\n",
        "\n",
        "To detect vertical edges, we would compute the derivative of the image with respect to $x$. Using the centered difference technique for numerical differentiation: $\\frac{\\partial F(x,y)}{\\partial x}\\approx\\frac{F(x+1,y)-F(x-1,y)}{2}$.\n",
        "\n",
        "Note that we are not actually numerically computing points of discontinuity in the gradient. Such a task is nontrivial. Rather, we compute the gradients over an image and expect the neural network to be able to \"intuit\" the discontinuities in gradient just as we are able to \"intuit\" discontinuities in graphs by observation.\n",
        "\n",
        "[[2]](#2)"
      ],
      "metadata": {
        "id": "343VepG1tzm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Filters and Sobel Operator\n",
        "In an image, we can estimate the horizontal and vertical gradient at some position using something similar to the center-difference operator applied as an image filter.\n",
        "\n",
        "If we take the estimated horizontal gradient at a point in an image to be the sum of the three points on its right minus the sum of the three points on its left and do something similar for the vertical gradient estimator, we get:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1&0&1\\\\\n",
        "-1&0&1\\\\\n",
        "-1&0&1\n",
        "\\end{bmatrix},\n",
        "\\begin{bmatrix}\n",
        "-1&-1&-1\\\\\n",
        "0&0&0\\\\\n",
        "1&1&1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "To clarify, the application of an image filter essentially performs dot products between the filters and local regions of the input. The process is visualized below. Another animation is in [[3]](#3).\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/90/CNN-filter-animation-1.gif\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Perhaps it would make sense to put a greater weight on the points to the immediate left and right of our target point. The Sobel operator does just this. The horizontal and vertical Sobel filters look something like:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "-1&0&1\\\\\n",
        "-2&0&2\\\\\n",
        "-1&0&1\n",
        "\\end{bmatrix},\n",
        "\\begin{bmatrix}\n",
        "-1&-2&-1\\\\\n",
        "0&0&0\\\\\n",
        "1&2&1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "You may see the Sobel operator with flipped signs. Ultimately, the magnitude of the gradient is what matters, and the order of the signs are just a technicality."
      ],
      "metadata": {
        "id": "izjrdsTpt08V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example\n",
        "Here is an example of applying the horizontal edge image filter.\n"
      ],
      "metadata": {
        "id": "yjhrUV_YxdWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image = np.array([\n",
        "    [1, 1, 1, 0, 0, 0]\n",
        "]*6)\n",
        "\n",
        "print(image)\n",
        "\n",
        "plt.imshow(image, cmap=mpl.colormaps['gray'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "r7X8xk_At40E",
        "outputId": "78bea900-c65a-4670-ad24-c76d8b97d5ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0]\n",
            " [1 1 1 0 0 0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter = np.array([\n",
        "    [-1, 0, 1],\n",
        "    [-2, 0, 2],\n",
        "    [-1, 0, 1]\n",
        "])\n",
        "\n",
        "print(filter)\n",
        "\n",
        "plt.imshow(filter, cmap=mpl.colormaps['gray'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "3VNtuDi7vIB3",
        "outputId": "97bf84f8-4351-475a-e86f-2661dc75b10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1  0  1]\n",
            " [-2  0  2]\n",
            " [-1  0  1]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SciPy includes a function to apply image filters."
      ],
      "metadata": {
        "id": "5FD-w8k3Vv3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "\n",
        "result = ndimage.correlate(image, filter)\n",
        "result = np.abs(result)\n",
        "\n",
        "print(result)\n",
        "\n",
        "plt.imshow(result, cmap=mpl.colormaps['gray'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "Tj4RSgZ4vOM2",
        "outputId": "8fb0c6ff-5a58-41d9-aeb0-bb57b1c96710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 4 4 0 0]\n",
            " [0 0 4 4 0 0]\n",
            " [0 0 4 4 0 0]\n",
            " [0 0 4 4 0 0]\n",
            " [0 0 4 4 0 0]\n",
            " [0 0 4 4 0 0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the horizontal Sobel filter, we were able to detect a vertical edge!\n",
        "\n",
        "Perhaps we can feed these representations of edges into a neural network, rather than the image itself, providing the network with refined data to work with.\n",
        "\n",
        "[[2]](#2), [[3]](#3), [[5]](#5), [[6]](#6)"
      ],
      "metadata": {
        "id": "KGREzKvHXtxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminology Clarification\n",
        "What we have seen as the application of a filter onto an image is known as cross-correlation. As we will see soon, this cross-correlation operation is part of the essence of CNNs.\n",
        "\n",
        "Similarly, there exists the convolution operation. The difference between the two is slight: in a convolution the filter (kernel) is flipped over both axes before being applied to the image. Mathematically, convolutions have nicer properties. Computationally, the flipping of the filter is unnecessary.\n",
        "\n",
        "Thus, the name of convolutional neural networks is a slight misnomer. However, the operation is referred to as a convolution by convention. Moving forwards, we will also follow this convention.\n",
        "\n",
        "(Hence, we used `scipy.ndimage.correlate` instead of `scipy.ndimage.convolve` above)\n",
        "\n",
        "[[7]](#7)"
      ],
      "metadata": {
        "id": "cR3u9GLiWlZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application to Neural Networks\n",
        "Using the Sobel operators, we can identify edges in the horizontal and vertical directions. However, how do we know if these edges extract the best representation of the image, rather than edges in some other direction?\n",
        "\n",
        "We could hand-craft and test different filters. Or, we can use neural networks to learn the best filters, making the weights of filters parameters that the network can learn.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w_1&w_2&w_3\\\\\n",
        "w_4&w_5&w_6\\\\\n",
        "w_7&w_8&w_9\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "kvPV5JreZrXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutions in Neural Networks\n",
        "Convolutions in neural networks are applied using convolution (conv) layers. A conv layer accepts a volume (image) of size $W_\\text{in}\\times H_\\text{in}\\times C_\\text{in}$ and produces a volume of size $W_\\text{out}\\times H_\\text{out}\\times C_\\text{out}$, where\n",
        "$$\n",
        "W_\\text{out}=\\lfloor\\frac{W_\\text{in}+2p-F}{S}\\rfloor+1\\\\\n",
        "H_\\text{out}=\\lfloor\\frac{H_\\text{in}+2p-F}{S}\\rfloor+1\\\\\n",
        "C_\\text{out}=K\n",
        "$$\n",
        "and $\\lfloor x\\rfloor$ is the floor function.\n",
        "\n",
        "If the filter is not contained within the image for some stride, the next step is taken without applying the filter. Others (e.g. [[3]](#3)) may consider a decimal quotient (without the floor) in width or height to be invalid for that reason.\n",
        "\n",
        "A visualization of the application of a conv layer is given in [[3]](#3). A snapshot of the visualization is given below.\n",
        "<div>\n",
        "<img src=\"https://g-2aaf39.eeb47.a567.data.globus.org/stanford%20conv%20vis.png\" width=\"700\"/>\n",
        "</div>\n",
        "\n",
        "[[2]](#2), [[3]](#3)"
      ],
      "metadata": {
        "id": "2KFMTXlab4ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "A conv layer has four hyperparameters:\n",
        "- $K$, the number of filters\n",
        "- $F$, the spatial extent\n",
        "- $S$, the stride\n",
        "- $P$, the amount of padding\n"
      ],
      "metadata": {
        "id": "Tlip9X1xiyp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Filters\n",
        "Each filter has $C_\\text{in}$ channels which convolves each of the $C_\\text{in}$ input channels and takes their sum with a bias parameter to produce one output channel.\n",
        "\n",
        "The input has $K$ filters applied to it, producing $K$ output channels.\n"
      ],
      "metadata": {
        "id": "68aifhTgi1Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spatial Extent\n",
        "The spatial extent of a filter refers to its size.\n"
      ],
      "metadata": {
        "id": "YUy2zBy4l2Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stride\n",
        "Stride refers to the number of elements the filter moves.\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1000/format:webp/1*34_365CJB5seboQDUrbI5A.gif\" width=\"500\"/>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "mISEvLNKl786"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding\n",
        "Padding refers to the additional space added around the image to be convolved. This additional space is typically filled in with zeros, though there are other strategies.\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gXAcHnbTxmPb8KjSryki-g.gif\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "mOoPBhqel9j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[1]](#1), [[3]](#3)"
      ],
      "metadata": {
        "id": "1Lfo43lMl_8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of Parameters of Conv Layer\n",
        "Each filter has $F^2\\cdot C_\\text{in}$ weights. For $K$ filters, there are $K\\cdot F^2\\cdot C_\\text{in}$ weights.\n",
        "\n",
        "There is also one bias for each filter, resulting in $K$ biases.\n",
        "\n",
        "In total, one conv layer has $K\\cdot F^2\\cdot C_\\text{in}+K$ parameters."
      ],
      "metadata": {
        "id": "NW5pfzv6nFNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrast with Parameters of Fully-Connected Layer\n",
        "Consider feeding images of size $n\\times n$ with $C_\\text{in}$ channels into a traditional neural network. A single fully-connected neuron in the first layer of the network would require $C_\\text{in}\\cdot n^2$ weight parameters. Thus, the space required for the weights will increase rapidly as $n$ increases, and this is just for one hidden neuron. For example, for $n=256,c=3$, $3*256^2=196,608$ weights are required for each hidden neuron.\n",
        "\n",
        "Contrast this with the $K\\cdot F^2\\cdot C_\\text{in}$ weights required for a conv layer. By inspection, one can see that there are far fewer parameters than that of a fully-connected layer.\n",
        "\n",
        "The large number of parameters of a fully-connected layer may be wasteful and could lead to overfitting."
      ],
      "metadata": {
        "id": "7PebpmmG5F21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[[2]](#2)"
      ],
      "metadata": {
        "id": "K5JzGPoj8bVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Connectivity\n",
        "The comparatively sparse connections of conv layers are because each neuron is connected only to a local region of the input volume, rather than all of the input volume.\n",
        "\n",
        "The spatial extent of this connectivity is called the **receptive field** of the neuron.\n",
        "\n",
        "The receptive field of a neuron in the layer immediately preceding it is simply the filter size. However, the receptive field of a neuron increases with the number of layers that precede it.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1200/1*k97NVvlMkRXau-uItlq5Gw.png\" width=\"500\"/>\n",
        "</div>\n",
        "For a filter with $F=3$, the receptive field of a neuron is $3\\times3$ in the layer before it and $5\\times5$ in the layer before that one.\n",
        "\n",
        "[[2]](#2), [[3]](#3)\n",
        "\n"
      ],
      "metadata": {
        "id": "uCeUf6pk49Xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Sharing\n",
        "As our CNN learns, the filters of each conv layer can be imagined as \"feature detectors\" that can extract the same feature (e.g. a vertical edge) from any part of the image. Thus, the parameters of a filter are used across the entire input rather than just a part of it.\n",
        "\n",
        "[[2]](#2), [[3]](#3)"
      ],
      "metadata": {
        "id": "GU8uhiEF8g5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization of Convolution Layer\n",
        "We've seen how to manually convolve an image using a conv layer made up of filters made up of channels. This operation was similar to a dot product between the filters and the local regions of the input. A common implementation of conv layers take advantage of this fact to formulate the forward pass of a conv layer to a single matrix multiplication:\n",
        "1. A matrix representation of the input is formed. Each local region, of size $F\\times F\\times C_\\text{in}$, is flattened into a column vector of size $F^2C_\\text{in}$, resulting in $W_\\text{out}H_\\text{out}$ column vectors. These column vectors are arranged into a matrix, $X_\\text{col}$, of size $F^2C_\\text{in}\\times W_\\text{out}H_\\text{out}$. This operation is commonly called **im2col**.\n",
        "2. A matrix representation of the filters composing the conv layer is formed. Each filter, of size $F\\times F\\times C_\\text{in}$, is flattened into a row vector of size $F^2C_\\text{in}$, resulting in $K$ row vectors for $K$ filters. These rows are arranged into a matrix, $W_\\text{row}$, of size $K\\times F^2C_\\text{in}$.\n",
        "3. $W_\\text{row} X_\\text{col}$, gives our result of size $K\\times W_\\text{out}H_\\text{out}$.\n",
        "4. This result must be reshaped back to $W_\\text{out}\\times H_\\text{out}\\times K$.\n",
        "\n",
        "Since receptive fields may overlap, values of the input may be repeated in the matrix representation of the input. Manual implementation may use a lot of memory but many efficient implementations exist.\n",
        "\n",
        "[[3]](#3)"
      ],
      "metadata": {
        "id": "upJWZzup95Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "Backpropagation of gradients in a conv layer is also a convolution, with spatially-flipped filters.\n",
        "\n",
        "[[2]](#2), [[3]](#3)"
      ],
      "metadata": {
        "id": "cQFEYQHQM1XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pooling\n",
        "Pooling layers are used to efficiently downsample images while retaining important information.\n",
        "\n",
        "Pooling layers:\n",
        "- Accept a volume of $W_\\text{in}\\times H_\\text{in}\\times C_\\text{in}$\n",
        "- Require two hyperparameters:\n",
        "  - $F$, the spatial extent\n",
        "  - $S$, the stride\n",
        "- Outputs a volume of $W_\\text{out}\\times H_\\text{out}\\times C_\\text{out}$, where\n",
        "$$\n",
        "W_\\text{out}=\\lfloor\\frac{W_\\text{in}-F}{S}\\rfloor+1\\\\\n",
        "H_\\text{out}=\\lfloor\\frac{H_\\text{in}-F}{S}\\rfloor+1\\\\\n",
        "C_\\text{out}=C_\\text{in}\n",
        "$$\n",
        "and $\\lfloor x\\rfloor$ is the floor function\n",
        "- Have no parameters\n",
        "\n",
        "There are multiple pooling strategies.\n",
        "\n",
        "\n",
        "[[3]](#3)\n",
        "\n"
      ],
      "metadata": {
        "id": "-5H9qPqYeuRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Min/Max Pooling\n",
        "Min/max pooling takes the min/max value in the spatial extent.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/MaxpoolSample2.png\" width=\"500\"/>\n",
        "</div>\n",
        "An example of max pooling with $F=2,S=2$"
      ],
      "metadata": {
        "id": "6SWXqAqJpWvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Pooling\n",
        "Average pooling takes the average of all values in the spatial extent.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png\" width=\"500\"/>\n",
        "</div>\n",
        "An example of average pooling with $F=2,S=2$"
      ],
      "metadata": {
        "id": "-KVuiHRctu6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L2-norm Pooling\n",
        "L2-norm pooling takes the L2-norm of the values in the spatial extent."
      ],
      "metadata": {
        "id": "Sol_2RZL37AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Rid of Pooling\n",
        "There is much discussion regarding the importance of pooling. [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806) proposes to discard pooling layers in favor of conv layers with larger strides.\n",
        "\n",
        "[[3]](#3)"
      ],
      "metadata": {
        "id": "OLVaaqXh3_jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architectures\n",
        "[[3]](#3)"
      ],
      "metadata": {
        "id": "xDySDJ5CYaBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Patterns\n",
        "The most common form of a CNN repeats a stack of a few CONV->RELU layers followed by a POOL layer, producing a reduced dimension image. This reduced dimension image is then flattened and fed to some number of fully-connected layers. In other words, CNNs often follow this pattern:\n",
        "\n",
        "`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`\n",
        "\n",
        "$N,M,K\\geq0$ and generally $N\\leq3,K<3$.\n"
      ],
      "metadata": {
        "id": "mZsIZrmyZ94u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Sizing Patterns\n",
        "Throughout the convolutional part of the network, generally, the width and height of the image is reduced (e.g. halved) while the depth is increased (e.g. doubled).\n",
        "\n",
        "Thus, the following patterns should be followed:\n",
        "- The input width and height should be divisible by 2 many times.\n",
        "- The conv layers should be using small filters (e.g. $K=3,5$), a stride of $1$, and selecting a padding size such that the conv layer is a **same convolution**, meaning that the width and height do not change from the input to the output.\n",
        "  - For example, for $F=3,S=1$, $P=1$ will maintain a same convolution because $D_\\text{out}=\\lfloor\\frac{D_\\text{in}+2(1)-3}{1}\\rfloor+1=D_\\text{in}$ for some dimension $D$.\n",
        "- Pool layers are responsible for downsampling the width and height of the input. Commonly, max-pooling is used with $F=2,S=2$. This discards $75\\%$ of the input activations.\n",
        "\n"
      ],
      "metadata": {
        "id": "nYYCgG0faAZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Study\n",
        "The above patterns emerged from empirical success.\n",
        "\n",
        "Let's follow the development of CNNs and observe some new techniques through a number of well-known architectures.\n",
        "\n",
        "Below shows the best Top-5 error of models submitted to ImageNet ILSVRC from 2010 to 2017.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://viso.ai/wp-content/uploads/2024/04/accuracy.png\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "[[2]](#2), [[3]](#3)"
      ],
      "metadata": {
        "id": "9c5-wcY8cTfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LeNet\n",
        "Presented by Yann LeCun in 1998, [LeNet](https://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) was one of the first successful applications of CNNs, used to read zip codes, digits, etc.\n",
        "\n",
        "<div>\n",
        "<img src=\"http://d2l.ai/_images/lenet.svg\" width=\"1000\"/>\n",
        "</div>\n",
        "\n",
        "1. A convolution of $F=5,S=1,P=0,K=6$ is applied to an input of $32\\times32\\times1$.\n",
        "2. Average pooling of $F=2,S=2$ is taken and a sigmoid activation is applied.\n",
        "3. A convolution of $F=5,S=1,P=0,K=16$ is applied.\n",
        "4. Average pooling of $F=2,S=2$ is taken and a sigmoid activation is applied.\n",
        "5. The result is flattened, then fed to a dense layer of $120$ neurons.\n",
        "6. After another dense layer, an output of $10$ neurons indicate which digit is being observed.\n",
        "\n",
        "We can compute the number of parameters with:\n",
        "$$\n",
        "\\underbrace{5\\cdot5\\cdot1\\cdot6+6}_{\\text{conv1}}+\n",
        "\\underbrace{5\\cdot5\\cdot6\\cdot16+16}_{\\text{conv2}}+\n",
        "\\underbrace{120\\cdot400+120}_{\\text{fc1}}+\n",
        "\\underbrace{84\\cdot120+84}_{\\text{fc2}}+\n",
        "\\underbrace{10\\cdot84+10}_{\\text{fc3}}=61706\n",
        "$$"
      ],
      "metadata": {
        "id": "s7dpPG3gdPRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AlexNet\n",
        "Developed by Alex Krizhevsky, et al., [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks) was the first work that popularized CNNs in computer vision. It was submitted to the ImageNet ILSVRC challenge in 2012 and won, significantly outperforming the runner-up. AlexNet was similar to LeNet, but was deeper, wider, and featured multiple conv layers before a pool.\n",
        "\n",
        "AlexNet (right) and LeNet (left) are compared below:\n",
        "<div>\n",
        "<img src=\"http://d2l.ai/_images/alexnet.svg\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "Skipping the computation, AlexNet has ~60 million parameters."
      ],
      "metadata": {
        "id": "L8nbZ62MhJF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ZFNet\n",
        "Developed by Zeiler and Fergus, [ZFNet](http://arxiv.org/abs/1311.2901) won ILSRVC 2013. It developed on AlexNet by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JFrrnbCkJjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGGNet\n",
        "Developed by Simonyan and Zisserman, [VGGNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) demonstrated that the depth of a CNN should be prioritized over filter size. Rather than using a single $7\\times7$ filter, one could use $3$ $3\\times3$ filters, resulting in an equivalent receptive field with fewer parameters ($49C^2>27C^2$). Thus, their network is composed of blocks of $3\\times3$ convolutions followed by a max pool.\n",
        "\n",
        "VGG16 won runner-up in ILSVRC 2014.\n",
        "\n",
        "Here is VGG16 and VGG19 compared with AlexNet.\n",
        "<div>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ZKtkYKJTT76-y_GtbFDhbg.jpeg\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "VGG16 has ~138 million parameters."
      ],
      "metadata": {
        "id": "ubnW77bAk-Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GoogLeNet\n",
        "From Szegedy et al., [GoogLeNet](http://arxiv.org/abs/1409.4842) developed and implemented the *Inception Module* and used Average Pooling instead of FC layers, significantly reducing the number of parameters.\n",
        "\n",
        "GoogLeNet won ILSVRC 2014.\n",
        "\n",
        "Its large architecture (22 layers) is shown in the paper. Despite it, GoogLeNet has just ~5 million parameters.\n"
      ],
      "metadata": {
        "id": "lRxpTlWUnc08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inception Module\n",
        "As seen in previous architectures, it is a good strategy to stack some \"module\" multiple times to compose a deep CNN. Instead of designing these modules ourselves, we can have our network learn them.\n",
        "\n",
        "To do so, we can apply different same convolutions to an input, and concatenate the outputs along the channel dimension.\n",
        "\n",
        "Examples of an Inception module are shown below.\n",
        "<div>\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_3.22.39_PM.png\" width=\"800\"/>\n",
        "</div>\n",
        "\n",
        "For example, suppose the following for the naive Inception module above:\n",
        "1. Input is of size $28\\times28\\times128$.\n",
        "2. There are $56$ $1\\times1$ filters.\n",
        "3. There are $100$ $3\\times3$ filters.\n",
        "4. There are $100$ $5\\times5$ filters.\n",
        "5. The max-pool layer produces $128$ channels.\n",
        "\n",
        "Then, concatenating the channels, the output is of size $28\\times28\\times384$. Computational complexity will be a problem. The Inception module with dimension reduction, above, mitigates this problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "E11zDbSApxqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet\n",
        "Developed by Kaiming He et al., [ResNet](http://arxiv.org/abs/1512.03385) developed and implemented residual connections and bottleneck layers, heavily utilized batchnorm, and has no FC layers at the end. It won ILSVRC 2015.\n",
        "\n",
        "Previously, the depth of a CNN had a limit, as at some point performance degraded. ResNet tackled this issue and was able to exhibit increasing performance with increasing depth, up to 152 layers.\n",
        "\n",
        "ResNet's architecture (34 layers) is illustrated in the paper."
      ],
      "metadata": {
        "id": "Rdr8d_rnsMF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batchnorm in CNNs\n",
        "Batchnorm allowed researchers to train ever-deeper networks. However, very-deep networks were strangely outperformed by less-deep networks.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://davidham3.github.io/blog/2018/03/04/deep-residual-learning-for-image-recognition/Fig1.PNG\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "Reasonably, a deep network should always outperform a less-deep network because the deep network could be initialized with the less-deep network and the remaining layers could act as an identify function."
      ],
      "metadata": {
        "id": "9SQ4lDcstP_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Residual Connections\n",
        "To assist the network in learning an identity function, we can make the output equal the input when the conv layers are zeroed.\n",
        "\n",
        "Such a residual (skip) connection is shown below.\n",
        "<div>\n",
        "<img src=\"https://davidham3.github.io/blog/2018/03/04/deep-residual-learning-for-image-recognition/Fig2.PNG\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "$F(x)$ is the output of the input after the first convolution, relu, and second convolution, but before the final activation."
      ],
      "metadata": {
        "id": "Ix25-wQktNVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bottleneck Layer\n",
        "The residual block on the left is modified to use a stack of 3 layers instead of 2. The $1\\times1$ layers first reduce, then increase (restore) the channels. The $3\\times3$ layer operates on the between, reduced dimension input.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://i.sstatic.net/kbiIG.png\" width=\"700\"/>\n",
        "</div>\n",
        "\n",
        "The block on the left requires $18HWC^2$ FLOPs while the block on the right requires $17HWC^2$. Thus, we can increase depth without increasing computational complexity."
      ],
      "metadata": {
        "id": "g6kV9Tzyv8bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "In general, ResNet is a popular choice for spatial performance due to its simplicity and performance. It is relatively straightforwards to apply ResNet to your own problem using [transfer learning](https://colab.research.google.com/drive/17Zz7WzqAadtAIHTSTkRMRf_xSgwBaTlr?usp=sharing).\n",
        "\n",
        "There are many popular models that have been developed after 2015. They will not be covered here.\n",
        "\n",
        "ResNet variants:\n",
        "- [ResNeXt](https://arxiv.org/abs/1611.05431) (2016)\n",
        "- [DenseNet](https://arxiv.org/abs/1608.06993) (2016)\n",
        "- [WideResNet](https://arxiv.org/abs/1605.07146) (2016)\n",
        "\n",
        "Effiency-focused models:\n",
        "- [MobileNet](https://arxiv.org/abs/1704.04861) (2017)\n",
        "- [ShuffleNet](https://arxiv.org/abs/1707.01083) (2017)\n",
        "- [EfficientNet](https://arxiv.org/abs/1905.11946) (2019)\n",
        "\n",
        "[[3]](#3)"
      ],
      "metadata": {
        "id": "H9aGlMory9jY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "1. T. Lee, V. P. Singh, K. H. Cho. 2021. Deep Learning for Hydrometeorology and Environmental Science. Available: https://content.e-bookshelf.de/media/reading/L-15184113-784f071c38.pdf\n",
        "2. H. Venkateswara. Introduction to Deep Learning. 2024.\n",
        "3. A. Karpathy, et. al. CS231n: Convolutional Networks. 2021. Available: https://cs231n.github.io/convolutional-networks/\n",
        "4. A. Devineni. How Hubel and Wiesel Revolutionized Neuroscience and Made Me a Neuroscientist. 2015. Available: https://www.brains-explained.com/how-hubel-and-wiesel-revolutionized-neuroscience/\n",
        "5. D. Marshall. Gradient based methods. 1997. Available: https://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MARSHALL/node28.html\n",
        "6. I. Sobel. History and Definition of the Sobel Operator. 2014. Available: https://www.researchgate.net/publication/239398674_An_Isotropic_3x3_Image_Gradient_Operator\n",
        "7. A. Rosebrock. Convolution and cross-correlation in neural networks. 2021. Available: https://pyimagesearch.com/2021/05/14/convolution-and-cross-correlation-in-neural-networks/"
      ],
      "metadata": {
        "id": "h-oxzt2cVc3J"
      }
    }
  ]
}